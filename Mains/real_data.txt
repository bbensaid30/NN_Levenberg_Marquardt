    omp_set_num_threads(omp_get_num_procs());
    //omp_set_num_threads(1);
    Eigen::initParallel();

   //Paramètres généraux
    std::string const distribution="Xavier";
    std::vector<double> const supParameters={-10,10};
    int const tirageMin=0;
    int const nbTirages=16;
    std::string const famille_algo="SGD";
    std::string const algo="Adam_bias";
    Sdouble const eps=std::pow(10,-2);
    int const maxIter=10000;

    //Paramètres des méthodes LM
    Sdouble mu=100, factor=10, RMin=0.25, RMax=0.75, epsDiag=std::pow(10,-16), Rlim=std::pow(10,-4), factorMin=std::pow(10,-8), power=1.0, alphaChap=1.1, alpha=0.75, pas=0.1;
    Sdouble tau=1, beta=2.0, gamma=3.0;
    int const b=1, p=3;

    Sdouble learning_rate=0.001;
    Sdouble clip=1/learning_rate;
    Sdouble seuil=0.01;
    Sdouble beta1 = 1-0.9;
    Sdouble beta2 = 1-0.999;

    int const nbPoints=550; Sdouble percTrain=0.1;
    int const PTrain = 110, PTest=990;
    int const batch_size = PTrain;
    std::vector<Eigen::SMatrixXd> dataTrain(2);
    std::vector<Eigen::SMatrixXd> data(4);
    dataTrain = twoSpiral(nbPoints);
    data = trainTestData(dataTrain,percTrain,true);
    //data = California(PTrain,PTest);
    //data = MNIST(PTrain,PTest);

    int const n0=data[0].rows(), nL=data[1].rows();
    int N=0;
    int const L=4;
    std::vector<int> nbNeurons(L+1);
    std::vector<int> globalIndices(2*L);
    std::vector<std::string> activations(L);
    std::vector<Eigen::SMatrixXd> weights(L);
    std::vector<Eigen::SVectorXd> bias(L);

    //Architecture
    int l;
    nbNeurons[0]=n0;
    for(int l=1; l<L; l++){nbNeurons[l]=12;}
    nbNeurons[L]=nL;
    for(int l=0;l<L-1;l++){activations[l]="tanh";}
    activations[L-1]="sigmoid";
    for(l=0;l<L;l++)
    {
        N+=nbNeurons[l]*nbNeurons[l+1]; globalIndices[2*l]=N; N+=nbNeurons[l+1]; globalIndices[2*l+1]=N;
    }

    std::string const type_perte = "norme2";
    std::vector<std::map<std::string,Sdouble>> studies(nbTirages);
     studies = tiragesClassification(data,l,nbNeurons,globalIndices,activations,type_perte,
    famille_algo,algo,supParameters,distribution,tirageMin,nbTirages,eps,maxIter,
    learning_rate,clip,seuil,beta1,beta2,batch_size,mu,factor,RMin,RMax,b,alpha,pas,Rlim,factorMin,power,alphaChap,epsDiag,true);
    std::string folder = "twoSpiral";
    std::string const fileEnd = informationFile(PTrain,PTest,L,nbNeurons,activations,type_perte,algo,supParameters,distribution,tirageMin,nbTirages,learning_rate,eps,maxIter);
    minsRecordClassification(studies,folder,fileEnd,eps);
    
    ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    
    omp_set_num_threads(omp_get_num_procs());
    //omp_set_num_threads(1);
    Eigen::initParallel();

   //Paramètres généraux
    std::string const distribution="uniform";
    std::vector<double> const supParameters={-100,100};
    int const tirageMin=0;
    int const nbTirages=1;
    std::string const famille_algo="Perso";
    std::string const algo="PGD";
    Sdouble const eps=std::pow(10,-2);
    int const maxIter=60000;

    //Paramètres des méthodes LM
    Sdouble mu=100, factor=10, RMin=0.25, RMax=0.75, epsDiag=std::pow(10,-16), Rlim=std::pow(10,-4), factorMin=std::pow(10,-8), power=1.0, alphaChap=1.1, alpha=0.75, pas=0.1;
    Sdouble tau=1, beta=2.0, gamma=3.0;
    int const b=1, p=3;

    Sdouble learning_rate=0.1;
    Sdouble clip=1/learning_rate;
    Sdouble seuil=0.01;
    Sdouble beta1 = 1-0.9;
    Sdouble beta2 = 1-0.999;

    int const nbPoints=100; Sdouble percTrain=0.5;
    int const PTrain = 50, PTest=50;
    int const batch_size = PTrain;
    std::vector<Eigen::SMatrixXd> dataTrain(2);
    std::vector<Eigen::SMatrixXd> data(4);
    dataTrain = squareWave(nbPoints);
    data = trainTestData(dataTrain,percTrain,true);
    //data = California(PTrain,PTest);
    //data = MNIST(PTrain,PTest);

    int const n0=data[0].rows(), nL=data[1].rows();
    int N=0;
    int const L=2;
    std::vector<int> nbNeurons(L+1);
    std::vector<int> globalIndices(2*L);
    std::vector<std::string> activations(L);
    std::vector<Eigen::SMatrixXd> weights(L);
    std::vector<Eigen::SVectorXd> bias(L);

    //Architecture
    int l;
    nbNeurons[0]=n0;
    for(int l=1; l<L; l++){nbNeurons[l]=15;}
    nbNeurons[L]=nL;
    for(int l=0;l<L-1;l++){activations[l]="sigmoid";}
    activations[L-1]="linear";
    for(l=0;l<L;l++)
    {
        N+=nbNeurons[l]*nbNeurons[l+1]; globalIndices[2*l]=N; N+=nbNeurons[l+1]; globalIndices[2*l+1]=N;
    }

    std::string const type_perte = "norme2";
    std::map<std::string,Sdouble> study;
    initialisation(nbNeurons,weights,bias,supParameters,distribution,50);
    study = train(data[0],data[1],L,nbNeurons,globalIndices,activations,weights,bias,type_perte,famille_algo,algo,eps,maxIter,
            learning_rate,clip,seuil,beta1,beta2,batch_size,
            mu,factor,RMin,RMax,b,alpha,pas,Rlim,factorMin,power,alphaChap,epsDiag,true,false,false);
    std::cout << "E: " << study["prop_entropie"] << std::endl;
    std::cout << "gradientNorm: " << study["finalGradient"] << std::endl;
    std::cout << "costFinal: " << study["finalCost"] << std::endl;
    std::cout << "gradientPrec: " << study["finalGradient"].digits() << std::endl;
    std::cout << "costFinalPrec: " << study["finalCost"].digits() << std::endl;
    std::cout << "iterTot: " << study["iter"] << std::endl;
    std::cout << "iterForward: " << study["iterForward"] << std::endl;
    if(numericalNoise(study["finalGradient"])){std::cout << "bruit" << std::endl;}
    
//---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
omp_set_num_threads(omp_get_num_procs());
    //omp_set_num_threads(1);
    Eigen::initParallel();

   //Paramètres généraux
    std::string const distribution="Xavier";
    std::vector<double> const supParameters={-10,10};
    int const tirageMin=0;
    int const nbTirages=2;
    std::string const famille_algo="Perso";
    std::string const algo="LC_EGD";
    Sdouble const eps=std::pow(10,-4);
    int const maxIter=200000;

    //Paramètres des méthodes LM
    Sdouble mu=100, factor=10, RMin=0.25, RMax=0.75, epsDiag=std::pow(10,-16), Rlim=std::pow(10,-4), factorMin=std::pow(10,-8), power=1.0, alphaChap=1.1, alpha=0.75, pas=0.1;
    Sdouble tau=1, beta=2.0, gamma=3.0;
    int const b=1, p=3;

    Sdouble learning_rate=0.1;
    Sdouble clip=1/learning_rate;
    Sdouble seuil=0.01;
    Sdouble beta1 = 1-0.9;
    Sdouble beta2 = 1-0.999;

    //int const nbPoints=550; Sdouble percTrain=0.1;
    int const PTrain = 11272, PTest=2818;
    int const batch_size = PTrain;
    std::vector<Eigen::SMatrixXd> dataTrain(2);
    std::vector<Eigen::SMatrixXd> data(4);
    //dataTrain = twoSpiral(nbPoints);
    //data = trainTestData(dataTrain,percTrain,true);
    data = California(PTrain,PTest);
    //data = MNIST(PTrain,PTest);

    int const n0=data[0].rows(), nL=data[1].rows();
    int N=0;
    int const L=2;
    std::vector<int> nbNeurons(L+1);
    std::vector<int> globalIndices(2*L);
    std::vector<std::string> activations(L);
    std::vector<Eigen::SMatrixXd> weights(L);
    std::vector<Eigen::SVectorXd> bias(L);

    //Architecture
    int l;
    nbNeurons[0]=n0;
    for(int l=1; l<L; l++){nbNeurons[l]=15;}
    nbNeurons[L]=nL;
    for(int l=0;l<L-1;l++){activations[l]="tanh";}
    activations[L-1]="linear";
    for(l=0;l<L;l++)
    {
        N+=nbNeurons[l]*nbNeurons[l+1]; globalIndices[2*l]=N; N+=nbNeurons[l+1]; globalIndices[2*l+1]=N;
    }

    std::string const type_perte = "norme2";
    std::vector<std::map<std::string,Sdouble>> studies(nbTirages);
    studies = tiragesRegression(data,l,nbNeurons,globalIndices,activations,type_perte,
    famille_algo,algo,supParameters,distribution,tirageMin,nbTirages,eps,maxIter,
    learning_rate,clip,seuil,beta1,beta2,batch_size,mu,factor,RMin,RMax,b,alpha,pas,Rlim,factorMin,power,alphaChap,epsDiag,true);
    
    for(int i=tirageMin; i<tirageMin+nbTirages; i++)
    {
        std::cout << "grPrec: " << numericalNoise(studies[i]["finalGradient"]) << std::endl;
        std::cout << "costTrain: " << studies[i]["finalCost"] << std::endl;
        std::cout << "costTest: " << studies[i]["cost_test"] << std::endl;
        std::cout << "iter: " << studies[i]["iter"] << std::endl;
        std::cout << "time: " << studies[i]["time"] << std::endl;
    }
